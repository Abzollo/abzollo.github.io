---
layout: post
title:  "Extrapolation as Interpolation"
date:   2020-11-30 00:21:19 +0400
---

*[These are only personal thoughts and are not backed by evidence.]*

Extrapolation of N observations to K new observations is just an interpolation of N+K observations such that N of them are "known”, and K of them are assumed to be generated or sampled from a generative process that can be (approximately) inferred from the known N observations.

We can hypothesize a structure that the N observations follow and impose it on the K unobserved points. For example, it helps to know whether the N observations lie in a space that is smooth, 1-Lipschitz, cyclical, bounded, low-dimensional, isomorphic to another space with a known structure, generated by a known process, etc. These things regularize our predictions and minimize the space of possibilities tremendously. They are meta-structures of the space of possible structures that these points lie in, so to speak (and can be so in a hierarchical way). We can also think about it this way: there is a meta-generative process that generates the structure of the space, and then there is another generative process that generates the K points based on this generated structure, so the generative process itself could be parameterized based on another generative process (or probability distribution). Extrapolation can be done by generating the points (K) outside of the interpolation regime (N) assuming they follow the same structure. Then, we can interpolate through all of the points together. The interpolation in the interpolation regime can be done "perfectly" as we know that the N points are given "as is" and are assumed to be "true" in some sense (with some known or unknown margin of error), but in the extrapolation regime it depends on how well the points were generated and how perfectly we can infer the structure from the given N points (which is directly correlated with the degree of error in observing the N points). It could be the case that the structure we need to infer from the N points in order to extrapolate well to the K points is just difficult to figure out. Anyway, this is all just a more complicated way of saying that prediction is a cyclical process of refined induction (inferring the structure) followed by deduction (deriving the predictions given the structure).

Let’s take a closer look at a real world example. Let’s say that recognizing a new face is done through interpolating the face to the space of face embeddings in our brain. Once we see something that does not immediately resemble a face we know (unconsciousness didn’t work?), we start to include reasoning and thinking in recognizing this face by actively retrieving similar faces or memories of things that resemble this face in a way that involves finer feature recognition (fine details of face, hair, eyes, number of teeth, a specific mark and its position on the face, etc.) Here, we are providing more structure to the space of face embeddings that we know are "true" and can be observed very clearly from the face, and this helps us generate the face embeddings that resemble the face we are observing even more closely. Once we have converged to a face embedding through this iterative recognition-refining process (or whatever you may call it), we either recognize the face by associating it with a very similar face embedding we already have in memory, or we learn this new face embedding and memorize it.

It could be the case that memorization is possible to the extent that one is able to reliably rewind the recognition process using a "significant” memory as a seed to regenerate this whole process. A "significant" memory is a memory that is significant enough to have been recognized and stored strongly in memory and retrieved by some sort of random access (maybe by "randomly" accessing memories relevant to the observed context, where randomly is loosely defined as the brain does not have a generator of true randomness... or does it?) Retrieving this memory helps retrieving the one that comes after it, and so on.

Ok, so what does all of this have anything to do with "interpolation as extrapolation”? Well, I don’t really know, but I think the relationship between interpolation and extrapolation is very similar to the one between "remembering” and "learning”. Learning has that extra step the same way extrapolation does.

Suppose you want to learn two new words. The first one is "charb” and the second is "turenate”. Let me tell you what my brain did when I first read these words (though what I did might be a bit biased because I came up with these words myself). As for the word "charb”, I noticed that it starts with "ch”, then I noticed that it is one of these sweet one-syllable words that are similar to "churn”, "charm”, "chart”, "barb”, "burp”, "carb”, "fart” etc. but starts with "ch”. For some reason, I feel like "churn” is the closest word to it. My brain makes weird decisions, but probably because my brain has 0 knowledge of its meaning, so it chose a word randomly (or maybe because of a slight bias in the way I pronounced it in my brain). I feel like it could be used this way: "he charbed in”, meaning "he interrupted the conversation in a smooth manner”. You know, something like that. But for all we know, this word could mean "swallow quickly, especially a large bite. Ex: my young brother charbed his whole sandwich!” As for the second word, which is "turenate”, I personally couldn’t associate it with many words, but for some reason I thought it sounded similar to "Turing-ate” (Turing is the name of a famous computer scientist and is also a technical term in computer science). Another term I think is close to it is "terminate”. Probably because I’m trying to recall words with similar start-end sounds. So for these reasons, I feel like "turenate” has some sort of a computational, mechanical connotation to it, even though it’s a completely made up word that could literally mean "decorate something with lights, usually a wall. Ex: my family turenated my bedroom and I hate it.” I guess an English native speaker would also find this word similar to "urinate”, but I haven’t used this word a lot in my life, to be honest with you, so it didn’t pop up in my head. But notice how close "urinate” is to "turenate”. The difference is almost the first letter only and they sound very similar, yet I completely missed it.

All of that is just the first few steps of recognizing a word, where I try to position and relate the word to my own knowledge and memory. Notice that I still never managed to actively memorize these words, and I can only recall them from the context of this whole experience of trying to recognize them. The process of recognizing a word is similar to interpolation in the sense that we are interpolating the word in the space of words we have in memory, given a (learned?) measure or words difference (which in my case used even sensory observations like how the word is pronounced and what it looks like). But really, how and when do we memorize a word? How do we learn it? How do we extrapolate it? That’s a really difficult question. Nobody can claim that they truly understand how the brain learns. It’s just a mystery. There is some connection between the concept of memory and knowledge, though. You remember the things you memorize the same way you know the things you learn. Learning has a lot to do with memorizing and remembering. It might be that, in order to learn something new, we have to hallucinate a memory to remember it as the new thing that we want to learn! Think about it. You can’t learn something new without creating some sort of a situation of having a memory of it in your brain. In other words, you can only learn something given its context (i.e. the things and memories that surround it). I mean, sometimes you have to be able to "generate” or simulate the possible scenarios and tell which ones make the most sense to you based on what you already know (i.e. most similar to what you have already seen).

Remember my examples about learning a new word. I always look for relevant features to make sense out of these completely new words. These features make logically no sense by the construction of these words themselves, which was a random process (with a little bit of bias from my brain). Still, my brain thinks that these are good features based on my previous experiences of learning words in general. When we use pure logic to learn something, we can only learn what we can directly infer from the facts that we already know, but we will never be able to understand how a completely new, seemingly-irrelevant observation relates to these facts unless we examine the partial similarities between them. This seems to be related to the way we try to understand the generative process of these facts, and then explain the similarities and differences based on this generative process. We can explain the common causes and effects, and summarize them and put them in memory. It’s like going up the ladder of abstraction, and seems like we can always go up and find shorter, more beautiful golden rules-of-thumb for any reasonable phenomenon that we observe in life. I mean, look at us now. We have reached extreme levels of abstractions such that we can explain how the physical world around us behave at a very fine scale with just a few equations. We have gone the extra mile in extrapolating the extrapolations.

I wish I could be more clear, but this is just a very abstract idea and I still don’t have the right words and thoughts to describe it properly, so I’m just going to end this essay at this point because I feel like if I say anything more after this it will start to become almost indistinguishable from noise.

